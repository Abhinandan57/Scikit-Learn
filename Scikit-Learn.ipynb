{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Machine Learning is a great tool to analyze data, find hidden data patterns and relationships, and extract information to enable information-driven decisions and provide insights. To do this Machine learning uses statistical and mathematical models and applies them to datasets.\n",
    "- Machine Learning terminologies:  \n",
    "__Observations:__ records, samples or examples present in the data. They may contain one or more data points.  \n",
    "__Features:__ Inputs or attributes that define a given dataset. They are usually present as columns in a spreadsheet.  \n",
    "__Response:__ Label, outcome, target or some defined answer attached to the dataset for the gievn set of data points.  \n",
    "- __Machine Learning Approach:__ The machine learning approach starts with either a problem that you need to solve or a given dataset that you need to analyze.  \n",
    "__(1)__ understand the problem/ dataset. The size of the dataset doesn't matter at this point.  \n",
    "__(2)__ identify or extract the features of the dataset that affect the outcome.  \n",
    "__(3)__ identify the problem type. For instance you may want to ask whether the data is categorical or some continuous set of values.  \n",
    "__(4)__ Based on the problem types choose the appropriate model.  \n",
    "__(5)__ After selecting the model train and test it.  \n",
    "__(6)__ Final setp is to strive for accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Machine Learning can either be __supervised__ or __unsupervised__. The problem type should be selected based on the type of learning model. \n",
    "- In supervised learning, the dataset used to create and train a model should have observations, features and responses. The model is trained to 'predict' the \"right\" response for a new set of data points with similar features. The main objective of this model is to predict the outcome by generalizing the data and applying the generalized rule to the new data points.\n",
    "- In unsupervised learning, the response or outcome of the data is unknown. So in this case we try to identify the patterns in the data by grouping similar types of data. The goal of this model is to 'represent' data in a way that meaningful information can be extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Problem Type:__  \n",
    "- Data can either be Continuous or Categorical. Based on whether it is supervised or unsupervised learning, the Problem type will differ.\n",
    "- For Supervised learning, use the __Regression__ algorithm if the data is Continuous. Use the __Classification__ learning algorithm if the data is Categorical. You can then build a predictive model based on top of the selected machine learning algorithm to predict the outcome of new observations.\n",
    "- For Unsupervised learning, use the __Dimensionality reduction__ if the data is continuous. This method will reduces the dimensions of the dataset without any loss of data. __Clustering__ is mainly used for categorical data & to group similar datapoints. Predictive model based on this model is used to identify the data patterns and represent it in a meaningful manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How it Works?__  \n",
    "- A known dataset has observations, which include features and response. In supervised learning, features and response are fed into the appropriate machine learning algorithm to train it. After the algorithm is fine tuned, Predictive model is built on top of it. Any new or unseen data with the same features will not have a label or response attached to it. The model is used to predict the response for this data.\n",
    "- In Unsupervised learning, a known dataset has a set of observations with features. But the response/outcome is unknown. Without the knowledge of what the expected outcome should be for a given set of data points, the machine learning algorithm can not be taught to predict the outcome for any new dataset. But you do know what the features are for a given dataset. Based on this information and your domain expertise, you can chose a few assumptions that will help you define the features or attributes the algorithm should watch out for.This helps us to identify, classify and visually represent any new or unseen data. You can also use cross validation to further test & train the model and improve its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Train, Test & Optimize the model:__  \n",
    "- The last two steps of machine learning approach involve testing, trainingand optimizing the model.\n",
    "- Only supervised learning models can be trained because all the right features and labels/responses are already known.\n",
    "- In unsupervised learning, the machine algorithm looks for similarities based only on statistical properties.\n",
    "- There are 2 approaches to train the supervised learning model. The first approach is to use two separate datasets, one to train the model and the other to test it. The second approach is to split a single dataset into two parts, a training set (60% - 80%) and a testing set(20% - 40%). Data analysts prefer the split approach because the algorithm uses the same set of data points for training and testing. And they can be changed from one iteration to the other. The split approach gives greater accuracy when it comes to predict the unknowns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While designing the supervised learning model, consider the following:  \n",
    "__(1)__ Response and the features which directly affect the outcome --> feed the machine learning algorithm and model directly  \n",
    "__(2)__ Fine tune the parameters of the model based on the training and testing results to optimize its performance and accuracy. This will help you to scale up the model easily.  \n",
    "__(3)__ Generalization means predicting the response. Strive for the model which can predict consistently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn\n",
    "- Scikit is a powerful and modern machine learning Python library. It a tool for fully and semi-automated data analysis and information extraction.\n",
    "- It has efficient tools to identify and organize problems such as whether data fits supervised or unsupervised learning model.\n",
    "- It contains many free and open datasets.\n",
    "- It has a rich set of built-in libraries for learning and predicting.\n",
    "- It provides model support for every problem type.\n",
    "- It also has built-in functions such as pickle for Model persistence.\n",
    "- It is supported by a huge open source community and vendor base.  \n",
    "__Scikit-Learn - Problem Solution Approach:__  \n",
    "(1) __Model Selection:__ Choosing the model and machine algorithm based on the dataset type.  \n",
    "(2) __Estimator Object:__ Using the __Estimator__ Object, which represents the model in Scikit-learn, by importing the class and instantiating it.  \n",
    "(3) __Model Training:__ Fitting the data into the model to train and test it.  \n",
    "(4) __Predictions:__ Using the __predict__ method to forcast the response of unseen or new dataset.  \n",
    "(5) __Model Tuning:__ Tunning the model through multiple iterations and result observations .  \n",
    "(6) __Accuracy:__ Striving for accuracy using built-in methods that supports the predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While working with a scikit-learn dataset or loading your own data to scikit learn, always consider these 4 points:  \n",
    "- create separate objects for feature and response.\n",
    "- ensure that features and response have only numeric values.\n",
    "- features and response should be in the form of a NumPy ndarray.\n",
    "- since features and response would be in the form of arrays, they would have shapes and sizes. Features are always mapped as 'x' and response is mapped as 'y'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning Models: Linear Regression\n",
    "- Linear regression is the most basic and widely used technique to predict a value of an attribute. It is pretty easy to use as the model doesn't require a lot of tuning. It also runs very fast, which makes it more time-efficient. \n",
    "- Scikit-learn has built-in linear regression model and will be using that for linear regression machine learning.\n",
    "- The regression line or the __least square line__ is the line of best fit.\n",
    "__y = mx + c__ is the Simple linear equation from Linear algebra. __y = β0 + β1x + u__ is the Linear regression equation which is derived from the simple linear equation.\n",
    "- Here y = Response, x = Input feature  \n",
    "__β0__ = __Intercept__ of the line, which equals to the value of the response variable when the input feature/predictor is 0.  It is the value of y when x = 0  \n",
    "__β1__ = __Coefficient__ of x (Solpe/gradient of the line). It is also called the slope parameter. It corresponds to the change in response variable to 1 unit change in predictor variable.  β1 = dy/dx\n",
    "__u__ = Residual value, comprises other factors which affect the prediction but not included in the model. It is the difference between the actual value and the predicted value of y i.e. u = y - (β0 + β1x) , which means that it equals the distance between the data point and the least square line.  \n",
    "- The attributes are usually fitted using the 'least square' approach.\n",
    "- ȳ indicates that the slope of the regression line is 0 and the intercept is the sample mean of the response variable.\n",
    "- SSR is indicates the difference between the regression line and the mean of the response variable.\n",
    "- SSE is the error of the sum of squares indicates the difference between the observed value and the regression line.  So Smaller the value of __SSR (Regression of sum of squares)__ or __SSE (Error of sum of squares)__, the more accurate the prediction will be, which would make the model the best fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__sklearn.linear_model.LinearRegression(fit_intercept = True, normalize = False, copy_X = True, n_jobs = 1)__  \n",
    "Linear Regression scikit-learn class has to be instantiated by creating an object, also called as __estimator__ i.e. sklearn.linear_model.LinearRegression(). \n",
    "- If the boolean variable __fit_intercept = True__, it calculates the intercept of the model. \n",
    "- When the boolean variable __normalize = True__, it normalizes the regression variable for performing the regression operation. \n",
    "- If __copy_X = True__, then the regression variable is copied. Else it will be overwritten, ended up a fault value is ended True.\n",
    "- __n_jobs__ sepecifies the number of jobs that are to be computed in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a built-in scikit-learn dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import scikit learn dataset\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "#creating an object to instantiate the dataset\n",
    "boston_dataset = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using built-in methods to explore and understand the data\n",
    "print(boston_dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n"
     ]
    }
   ],
   "source": [
    "#print the features of the dataset\n",
    "print(boston_dataset.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the data into a dataframe\n",
    "df_boston = pd.DataFrame(boston_dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set feature names as columns in the dataframe\n",
    "df_boston.columns = boston_dataset.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view first 5 observations\n",
    "df_boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n"
     ]
    }
   ],
   "source": [
    "#print dataset matrix (observations and features matrix)\n",
    "print(boston_dataset.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "#print dataset target or response shape\n",
    "print(boston_dataset.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 15.  18.9 21.7 20.4\n",
      " 18.2 19.9 23.1 17.5 20.2 18.2 13.6 19.6 15.2 14.5 15.6 13.9 16.6 14.8\n",
      " 18.4 21.  12.7 14.5 13.2 13.1 13.5 18.9 20.  21.  24.7 30.8 34.9 26.6\n",
      " 25.3 24.7 21.2 19.3 20.  16.6 14.4 19.4 19.7 20.5 25.  23.4 18.9 35.4\n",
      " 24.7 31.6 23.3 19.6 18.7 16.  22.2 25.  33.  23.5 19.4 22.  17.4 20.9\n",
      " 24.2 21.7 22.8 23.4 24.1 21.4 20.  20.8 21.2 20.3 28.  23.9 24.8 22.9\n",
      " 23.9 26.6 22.5 22.2 23.6 28.7 22.6 22.  22.9 25.  20.6 28.4 21.4 38.7\n",
      " 43.8 33.2 27.5 26.5 18.6 19.3 20.1 19.5 19.5 20.4 19.8 19.4 21.7 22.8\n",
      " 18.8 18.7 18.5 18.3 21.2 19.2 20.4 19.3 22.  20.3 20.5 17.3 18.8 21.4\n",
      " 15.7 16.2 18.  14.3 19.2 19.6 23.  18.4 15.6 18.1 17.4 17.1 13.3 17.8\n",
      " 14.  14.4 13.4 15.6 11.8 13.8 15.6 14.6 17.8 15.4 21.5 19.6 15.3 19.4\n",
      " 17.  15.6 13.1 41.3 24.3 23.3 27.  50.  50.  50.  22.7 25.  50.  23.8\n",
      " 23.8 22.3 17.4 19.1 23.1 23.6 22.6 29.4 23.2 24.6 29.9 37.2 39.8 36.2\n",
      " 37.9 32.5 26.4 29.6 50.  32.  29.8 34.9 37.  30.5 36.4 31.1 29.1 50.\n",
      " 33.3 30.3 34.6 34.9 32.9 24.1 42.3 48.5 50.  22.6 24.4 22.5 24.4 20.\n",
      " 21.7 19.3 22.4 28.1 23.7 25.  23.3 28.7 21.5 23.  26.7 21.7 27.5 30.1\n",
      " 44.8 50.  37.6 31.6 46.7 31.5 24.3 31.7 41.7 48.3 29.  24.  25.1 31.5\n",
      " 23.7 23.3 22.  20.1 22.2 23.7 17.6 18.5 24.3 20.5 24.5 26.2 24.4 24.8\n",
      " 29.6 42.8 21.9 20.9 44.  50.  36.  30.1 33.8 43.1 48.8 31.  36.5 22.8\n",
      " 30.7 50.  43.5 20.7 21.1 25.2 24.4 35.2 32.4 32.  33.2 33.1 29.1 35.1\n",
      " 45.4 35.4 46.  50.  32.2 22.  20.1 23.2 22.3 24.8 28.5 37.3 27.9 23.9\n",
      " 21.7 28.6 27.1 20.3 22.5 29.  24.8 22.  26.4 33.1 36.1 28.4 33.4 28.2\n",
      " 22.8 20.3 16.1 22.1 19.4 21.6 23.8 16.2 17.8 19.8 23.1 21.  23.8 23.1\n",
      " 20.4 18.5 25.  24.6 23.  22.2 19.3 22.6 19.8 17.1 19.4 22.2 20.7 21.1\n",
      " 19.5 18.5 20.6 19.  18.7 32.7 16.5 23.9 31.2 17.5 17.2 23.1 24.5 26.6\n",
      " 22.9 24.1 18.6 30.1 18.2 20.6 17.8 21.7 22.7 22.6 25.  19.9 20.8 16.8\n",
      " 21.9 27.5 21.9 23.1 50.  50.  50.  50.  50.  13.8 13.8 15.  13.9 13.3\n",
      " 13.1 10.2 10.4 10.9 11.3 12.3  8.8  7.2 10.5  7.4 10.2 11.5 15.1 23.2\n",
      "  9.7 13.8 12.7 13.1 12.5  8.5  5.   6.3  5.6  7.2 12.1  8.3  8.5  5.\n",
      " 11.9 27.9 17.2 27.5 15.  17.2 17.9 16.3  7.   7.2  7.5 10.4  8.8  8.4\n",
      " 16.7 14.2 20.8 13.4 11.7  8.3 10.2 10.9 11.   9.5 14.5 14.1 16.1 14.3\n",
      " 11.7 13.4  9.6  8.7  8.4 12.8 10.5 17.1 18.4 15.4 10.8 11.8 14.9 12.6\n",
      " 14.1 13.  13.4 15.2 16.1 17.8 14.9 14.1 12.7 13.5 14.9 20.  16.4 17.7\n",
      " 19.5 20.2 21.4 19.9 19.  19.1 19.1 20.1 19.9 19.6 23.2 29.8 13.8 13.3\n",
      " 16.7 12.  14.6 21.4 23.  23.7 25.  21.8 20.6 21.2 19.1 20.6 15.2  7.\n",
      "  8.1 13.6 20.1 21.8 24.5 23.1 19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9\n",
      " 22.  11.9]\n"
     ]
    }
   ],
   "source": [
    "#view target or response\n",
    "print(boston_dataset['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Train a Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import the boston dataset\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "#creating an object to instantiate the dataset\n",
    "boston_dataset = load_boston()\n",
    "print(boston_dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
      "       'PTRATIO', 'B', 'LSTAT'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#create a pandas dataframe and store the data\n",
    "df_boston = pd.DataFrame(boston_dataset.data)\n",
    "df_boston.columns = boston_dataset.feature_names\n",
    "print(df_boston.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Price of the houses in the Boston area is one of the columns of the dataframe\n",
    "#defining it as a new column as the target/response to the dataset\n",
    "df_boston['Price'] = boston_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  Price  \n",
       "0     15.3  396.90   4.98   24.0  \n",
       "1     17.8  396.90   9.14   21.6  \n",
       "2     17.8  392.83   4.03   34.7  \n",
       "3     18.7  394.63   2.94   33.4  \n",
       "4     18.7  396.90   5.33   36.2  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view top 5 observations\n",
    "df_boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign features on X-axis\n",
    "x_features = boston_dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign the target on Y-axis\n",
    "y_target = boston_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to use scikit-learn linear regression model import its class, create its object/estimator to instantiate the class\n",
    "#import linear model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lineReg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now that the model is created, fit the datasets both the features and target values into the model object\n",
    "lineReg.fit(x_features, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the estimated intercept 36.46 \n"
     ]
    }
   ],
   "source": [
    "#print the intercept (β0)\n",
    "print(\"the estimated intercept %.2f \"%lineReg.intercept_)  #%.2f is used to round off decimal number to 2 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the coefficient is 13 \n"
     ]
    }
   ],
   "source": [
    "#print the coefficient (β1)\n",
    "print(\"the coefficient is %d \"%len(lineReg.coef_))   #%d is used to print numerical or decimal variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model by importing class train_test_split\n",
    "#cross_validation class is deprecated since version 0.18. This module will be removed in 0.20.\n",
    "#Use sklearn.model_selection.train_test_split instead.\n",
    "\n",
    "#split the whole dataset into train and test datasets (by default split % of training & testing dataset is 75-25)\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_features, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n"
     ]
    }
   ],
   "source": [
    "#print the dataset shape\n",
    "print(boston_dataset.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 13) (127, 13) (379,) (127,)\n"
     ]
    }
   ],
   "source": [
    "#print shapes of the training and testing datasets\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the training dataset into the estimator/model object\n",
    "lineReg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE value is 19.09 \n"
     ]
    }
   ],
   "source": [
    "#calculate the mean square error (MSE) or residual sum of squares of the model using the predict method\n",
    "print(\"MSE value is %.2f \"%np.mean((lineReg.predict(x_test) - y_test)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance score is 0.75 \n"
     ]
    }
   ],
   "source": [
    "#calculate the variance using the score method - the closer the value to 1, the higher the model accuracy\n",
    "print(\"variance score is %.2f \"%lineReg.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning Models: Logistic Regression\n",
    "- Logistic regression algorithms are mainly used for categorical data.\n",
    "- We use a set of input features to predict the probabilities of class memberships. In simpler terms the algorithm predicts the class of each observation present in the dataset.\n",
    "- Odds of an event is the ratio of the probability of the event to its complement. If you take the logarithm of the odds it becomes linear regression. So the data generalization switches from categorical to regression.\n",
    "- LogisticRegression a scikit learn class has to be instantiated by creating an object or estimator.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__class sklearn.linear_model.LogisticRegression(penalty = 'l2', dual = False, tol = 0.0001, C = 1.0, fit_intercept = True, intercept_scaling = 1, class_weight = None, random_state = None, solver = 'liblinear', max_iter = 100, multi_class = 'ovr', verbose = 0, warm_start = False, n_jobs = 1)__  \n",
    "- There can be two types of algorithms, first is '__ovr__'(one vs. rest). It is most suitable for binary problems. The 2nd algorithm type is '__multi-nominal__', which is a __multi_class__ option. multi_class specifies whether the algorithm is 'ovr' or 'multi-nominal'.\n",
    "- __penalty__ is used to specify the norm used in penalization.\n",
    "- __dual__ is implemented only for a l2 penalty.\n",
    "- __C__ is the inverse of regularization.\n",
    "- __fit_intercept__ calculates the intercept.\n",
    "- If the boolean variable __warm_start__ is True, then the algorithm reuses the solution of the previous call.\n",
    "- __n_jobs__ specifies the number of jobs in parallel computation.\n",
    "- __random_state__ is the seed or the random state instance.\n",
    "- __liblinear__ is the algorithm to be used in the optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning Models: K Nearest Neighbors (K-NN)\n",
    "- K Nearest Neighbors (K-NN) is one of the simplest machine mearning algorithms used for both classification and regression problem types. This algorithm uses the entire training dataset to create its model.\n",
    "- It looks at the inputs or features or attributes of the training dataset to identify the attributes or any new unseen data. Based on how similar are the data points to an attribute or in simpler terms how near it is to the input data point, the algorithm classifies it.\n",
    "- Imagine we have two classes in our training set. Class A represented by red circles and Class B shown as blue circles. Let's mark a new unseen data point as a green circle. Now to classify it, we just expand a specific distance away from our feature space until we hit K number of other data points.\n",
    "- If you are using this method for binary classification, choose an odd number for k to avoid the case of a tied distance between two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The use of K-NN and Logistic regression models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary library\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sklearn load dataset\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "#creating an object to instantiate the dataset\n",
    "iris_dataset = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display the dataset type\n",
    "type(iris_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "#view information using dataset built-in method DESCR (describe)\n",
    "print(iris_dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "#view features\n",
    "print(iris_dataset.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "#view target or response\n",
    "print(iris_dataset.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "#find number of observations\n",
    "print(iris_dataset.data.shape)   #it shows that the dataset has 150 observations and 4 attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign features data to x-axis\n",
    "x_feature = iris_dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign target data to y-axis\n",
    "y_target = iris_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "#view the shape for both axes\n",
    "print(x_feature.shape)\n",
    "print(y_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first use KNN classifier method to find the nearest neighbors of the data point for a given value of K\n",
    "#import KNeighborsClassifier class\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate the class with knn object/estimator\n",
    "knn = KNeighborsClassifier(n_neighbors = 1)   \n",
    "#passing the k-neighbor value as 1, so that the estimator will look for the first nearest neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The object used to instantiate the class of a learning model is called an __estimator__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_neighbors=1)\n"
     ]
    }
   ],
   "source": [
    "#printing the estimator\n",
    "print(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the data into KNN model (estimator)\n",
    "knn.fit(x_feature, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing two new sets of values to the object x_new for which will predict the outcome\n",
    "x_new = [[3, 5, 4, 1],[5, 3, 4, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict the outcome for the new observation using knn classifier\n",
    "knn.predict(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the outcome using logistic regression model/estimator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logReg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit data into the logistic regression estimator\n",
    "logReg.fit(x_feature, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict the outcome using Logistic regression estimator\n",
    "logReg.predict(x_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning Model: K-Means Clustering\n",
    "- Clustering is the way to group similar data points.\n",
    "- It helps to extract the structure of the data, identify groups in the data.\n",
    "- Greater similarity between data points results in better clustering.\n",
    "- K-Means, one of the most popular clustering algorithms stores k centroids which are used to define clusters. A point is considered to be in a particular cluster, if it is closer to that cluster centroid than any other centroid.\n",
    "- K-means finds the best centroids by alternatively assigning random centroids to a dataset and selecting mean data points from the resulting clusters to form new centroids. It continues this process iteratively until the model is optimized.\n",
    "- You have to first understand the dataset. Make some assumptions about it based on your expertise and then feed those assumptions to the k-means clustering algorithm. Based on those inputs K-means randomly assigns a specific number of centroids to a dataset. It assigns the data points to the centroids depending on their proximity to them. Once the data points are grouped separately K-Means chooses a data point that is in the center of each group.Then it reassigns the data points to these new centroids based on their proximity. If this creates a new set of groups then K-Means again chooses a data point from the center of each group to form centroids. It repeats this process again and again until the model is optimized.\n",
    "- Scikit-learn has a class called KMeans. It has to be instantiated by creating an object, also called an estimator.\n",
    "__sklearn.cluster.KMeans(n_clusters = 8, init = 'k-means++', n_init = 10, max_iter = 300, tol = 0.0001, precompute_distances = 'auto', verbose = 0, random_state = None, copy_x = True, n_jobs = 1)__  \n",
    "- __n_clusters__ gives the number of clusters and centroids that are to be formed.\n",
    "- __init = 'k-means++'__ selects initial cluster centers.\n",
    "- __n_init__ specifies the no. of times the K-Means algorithm will be run with different centroid seeds.\n",
    "- __max_iter__ specifies the maximum number of iterations of the K-means algorithm for a single run.\n",
    "- __precompute_distances__ is used to pre-compute the algorithm for faster operation.\n",
    "- __random_state__ initializes the centers.\n",
    "- If the __copy_x__ argument is True, then the algorithm will not modify data when it carries out the pre-computation.\n",
    "- __n_jobs__ specifies the number of jobs in parallel computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means clustering to classify data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#import KMeans class from sklearn.cluster\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Along with its built-in datasets, scikit-learn also has __random sample generators__. They are useful in creating artificial datasets of a specified complexity and size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import make_blobs random sample generator from sklearn.datasets\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 2, 2, 0, 0, 2, 0, 1, 2, 1, 0, 1, 0, 1, 0, 0, 2, 0, 2, 2,\n",
       "       0, 0, 2, 0, 0, 0, 2, 2, 1, 1, 1, 1, 1, 2, 0, 1, 2, 2, 0, 2, 0, 1,\n",
       "       0, 1, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 0, 2, 1, 1, 1, 2,\n",
       "       0, 2, 2, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 2, 0, 1, 1, 0,\n",
       "       2, 1, 1, 2, 1, 2, 2, 1, 1, 0, 0, 0, 1, 2, 1, 1, 1, 2, 2, 0, 1, 1,\n",
       "       1, 2, 0, 2, 2, 1, 0, 1, 1, 1, 2, 1, 2, 0, 1, 2, 0, 2, 0, 1, 1, 2,\n",
       "       0, 1, 2, 2, 1, 0, 1, 0, 1, 1, 2, 2, 2, 2, 2, 0, 2, 0, 2, 0, 1, 2,\n",
       "       0, 2, 1, 1, 0, 1, 0, 0, 0, 1, 1, 2, 0, 2, 2, 2, 0, 2, 0, 2, 0, 0,\n",
       "       2, 1, 0, 1, 0, 2, 0, 2, 0, 0, 0, 0, 2, 0, 2, 0, 1, 1, 2, 0, 2, 0,\n",
       "       0, 1, 2, 0, 1, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 1, 2, 0, 0, 2, 1, 1,\n",
       "       2, 1, 1, 2, 2, 1, 1, 2, 0, 1, 2, 2, 2, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "       2, 1, 2, 1, 0, 1, 0, 2, 1, 2, 2, 1, 1, 2, 2, 0, 1, 1, 1, 0, 1, 2,\n",
       "       0, 0, 2, 2, 0, 1, 1, 0, 2, 2, 2, 0, 0, 2, 2, 2, 1, 1, 0, 1, 2, 1,\n",
       "       2, 1, 2, 1, 0, 0, 1, 0, 0, 0, 2, 2, 0, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now specify the properties of the dataset that you want to generate\n",
    "\n",
    "#define number of samples\n",
    "n_samples = 300\n",
    "#define random state value to initialize the center\n",
    "random_state = 20\n",
    "#define number of features as 5\n",
    "x, y = make_blobs(n_samples = n_samples, n_features = 5, random_state = None)\n",
    "#define number of cluster to be formed as 3 and in random state and fit features into the model\n",
    "predict_y = KMeans(n_clusters = 3, random_state = random_state).fit_predict(x)\n",
    "#print the estimator prediction\n",
    "predict_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning Model: Dimensionality Reduction\n",
    "- A large dataset can have multiple dimensions. When you have a multi-dimensional dataset, you need a lot of data points to create its representative sample. If you have to perform an analytics operation on it, the performance will be very slow and sluggish. For example, just imagine a large spreadsheet with a few thousand rows and a few thousand columns. You definitely want to trim down the spreadsheet for faster computation. To do that you might want to drop off a few columns from the dataset without losing the information it contains because that might led to a wrong analysis.This is where dimensionality reduction comes into play. It helps cut down the dimension of a multi-dimensional dataset without losing any of the data.\n",
    "- __Different techniques used for dimensionality reduction:__  \n",
    "__(1) Drop the columns which contain missing values and exceeds some defined threshold value for the dataset.__  \n",
    "__(2) Drop data columns with low variance.__ Data variance indicates the changes in the data value. So if there are small changes in the data then you can choose to remove data lower than the defined threshold after normalizing it.  \n",
    "__(3) Drop data columns with high correlations.__ Correlation indicates how data points are related to each other. If they are highly correlated, only one set of data points can be kept and all others can be dropped.  \n",
    "__(4) Apply statistical functions - PCA(Principle Component Analysis)__ PCA is a statistical function which transforms the original datasets into a new set of coordinates by keeping the highest possible variance to ensure that there is a significant loss in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning Model: Principal Component Analysis (PCA)\n",
    "- It is primarily used to compress or reduce the data.\n",
    "- PCA tries to capture the variance, which helps it pick up interesting features. That is why it is also useful as a feature extraction method.\n",
    "- PCA is used to reduce dimensionality in the dataset and to build our feature vector.\n",
    "- PCA is a linear dimensionality reduction method which uses singular value decomposition of the data and keeps only the most significant singular vectors to project the data to a lower dimensional space.\n",
    "- Scikit-Learn has a class called PCA.\n",
    "__sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False)__ This calss has to be instantiated by creating an object/estimator.\n",
    "- __n_components__ indicates the number of components that should be retained. If this variable is not set, then all the components are retained.\n",
    "- __copy__ overwrites the transform data after fitting them into the model. It is not used frequently, because we don't often need to overwrite the transformed data.\n",
    "- __whiten__ simply removes the data with lower variance and improves the prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA model to reduce the dimensions of a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required library PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "##import make_blobs random sample generator from sklearn.datasets\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now specify the properties of the dataset that you want to generate\n",
    "\n",
    "#for the sample generator define the size of the sample and its random state values\n",
    "n_sample = 20\n",
    "random_state = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the dataset with 10 features (dimension)\n",
    "x, y = make_blobs(n_samples=n_sample, n_features=10, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view the shape of the dataset\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.65056152 -6.20946939 -3.21690051 -3.84708088  7.1322747  -1.25300318\n",
      "  5.61150429  7.37125654  3.48741291 -7.75159673]\n"
     ]
    }
   ],
   "source": [
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.65056152  -6.20946939  -3.21690051  -3.84708088   7.1322747\n",
      "   -1.25300318   5.61150429   7.37125654   3.48741291  -7.75159673]\n",
      " [  4.55010154  -6.79329726  -1.17405525  -4.05566361   6.97003967\n",
      "   -2.37721483   5.76273805   6.22804688   3.51597407  -6.5435204 ]\n",
      " [ -7.08494579  -9.57910886  -0.80394279  -5.46059629   0.15977055\n",
      "   -1.16650593  -8.45574164  -6.49240505  -1.12829648  -2.69950347]\n",
      " [  6.28848132  -6.28818145  -3.92676813  -2.595785     7.73763314\n",
      "   -1.7901927    4.77308217   5.41238859   5.70380278  -6.78347847]\n",
      " [ -8.27948153  -9.47067181  -0.24643006  -6.36806356   2.61369548\n",
      "    0.82837346 -10.79189995  -5.48730554  -1.69455108  -1.97145644]\n",
      " [ -8.86049497  -5.29572228  -8.24517933   8.95591699  -5.39555238\n",
      "   -4.71630841 -11.02538703  -3.08926324  -9.20622632   0.86731101]\n",
      " [ -6.8805804   -9.11835942  -0.94797211  -8.04063957   1.92194649\n",
      "   -0.07826479  -8.67190152  -5.54359197  -0.45125379  -1.94660383]\n",
      " [ -6.65792837  -6.26925725  -5.70770018   8.23641147  -4.08257802\n",
      "   -3.49002643  -9.76072051  -2.45688915  -7.65167558   2.80921127]\n",
      " [ -8.04508502  -3.8709407   -6.73212692   8.88393135  -5.28828239\n",
      "   -4.09105149  -8.82134257  -1.76444536  -6.43735316   3.10496255]\n",
      " [ -5.95835876  -5.8783208   -5.59180128   8.09318611  -4.53160802\n",
      "   -3.2872258   -7.85777695  -2.51937306  -7.52282333   5.71773502]\n",
      " [ -6.51702774  -8.53863244  -0.47044628  -6.94313514   0.48801496\n",
      "   -0.71858455  -9.88832256  -6.07575032  -0.96506807  -3.4344801 ]\n",
      " [ -8.85125365  -4.57066962  -8.99105243   9.88522523  -3.27923236\n",
      "   -4.32602814  -8.59279179  -3.10939972  -8.52688964   3.10185654]\n",
      " [ -6.58312379  -9.83641639  -1.76536763  -7.6477148    3.08846965\n",
      "   -0.9946703  -10.38553028  -8.15043417   1.18038707  -3.06934343]\n",
      " [  5.92768108  -6.18475019  -3.35092449  -3.60299856   8.07408629\n",
      "   -2.8046498    7.10176137   6.21731823   5.75022135  -7.83338725]\n",
      " [ -7.17238427  -8.25283665  -1.60390883  -6.53281351   1.68448954\n",
      "   -0.82936443  -9.68513722  -5.47572209  -0.03666549  -2.03503235]\n",
      " [  6.02258645  -4.84581562  -3.70394402  -1.79179866   8.14314121\n",
      "   -1.47495243   5.78311384   5.73070066   5.00357099  -7.96071532]\n",
      " [  6.45008028  -8.04761635  -5.2722944   -3.31837768   8.12255671\n",
      "    0.02146698   5.02100512   6.48692818   4.48952811  -7.74782072]\n",
      " [ -5.01846994 -10.25327684   0.13406958  -4.56553581   0.43369003\n",
      "    0.18785724  -8.50254303  -5.19881636   0.40479786  -3.85104479]\n",
      " [  6.02991076  -5.45556276  -3.94887161  -3.80143499   7.57881999\n",
      "   -2.93954569   5.58440209   5.8036544    2.73791999  -7.87517344]\n",
      " [ -5.90936466  -6.01019673  -7.69972243   8.45029448  -4.4631462\n",
      "   -5.07822179 -10.36845425  -0.68398298  -8.38404807   3.94070135]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 1 2 1 2 2 2 1 2 1 0 1 0 0 1 0 2]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the PCA estimator with number of reduced components\n",
    "pca = PCA(n_components = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72540562 0.24887597 0.00699994]\n"
     ]
    }
   ],
   "source": [
    "#fit the data into the PCA estimator\n",
    "pca.fit(x)\n",
    "print(pca.explained_variance_ratio_)\n",
    "#explained_variance_ratio_ gives the % of variance explained by each of the selected component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44558074  0.00159935  0.0696526  -0.275885    0.36088182  0.04803442\n",
      "  0.51276808  0.31938217  0.35517232 -0.31123107]\n"
     ]
    }
   ],
   "source": [
    "#print the first PCA component\n",
    "first_pca = pca.components_[0]\n",
    "print(first_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the fitted data using transform method to apply the dimensionality reduction\n",
    "pca_reduced = pca.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view the reduced dataset shape (shape of lower dimension)\n",
    "pca_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "- Pipeline help simplify the process where more than one model is required or used. When you are using it, call the predict method once all the data is fit into the models or estimators.\n",
    "- All models in the pipeline execpt the last one must be transformers . The last model can either be a transformer or a classifier, regressor or other such objects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a pipeline containing more than one estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pipeline class\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#import linear estimator\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#import PCA estimator for dimensionality reduction\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chain the estimators together\n",
    "#All models in the pipeline execpt the last one must be transformers . \n",
    "#The last model can either be a transformer or a classifier, regressor\n",
    "estimator = [('dim_reduction',PCA()),('linear_model',LinearRegression())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a pipeline object and pass the chained estimator to it\n",
    "pipeline_estimator = Pipeline(estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('dim_reduction', PCA()), ('linear_model', LinearRegression())])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the chain of estimators\n",
    "pipeline_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dim_reduction', PCA())"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#viewing all the estimators in a sequencial order\n",
    "#steps method helps you to view step wise processing of the pipeline\n",
    "\n",
    "#view first step\n",
    "pipeline_estimator.steps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('linear_model', LinearRegression())"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view second step\n",
    "pipeline_estimator.steps[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dim_reduction', PCA()), ('linear_model', LinearRegression())]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view all the steps in pipeline\n",
    "pipeline_estimator.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Persistence\n",
    "- Once you create a model you should be able to save it as well, so that you can reuse it later. This is called Model Persistence.\n",
    "- It is possible to save a model by using Python's __Pickle__ method.\n",
    "- Scikit-learn has a special replacement for pickle called __joblib__. Using its __joblib.dump__ and __joblib.load__ methods, you can read and write estimator objects respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist a model for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import require libraries and dataset\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "#creating an instance of the dataset\n",
    "iris_dataset = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view feature names of the dataset\n",
    "iris_dataset.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view target of the dataset\n",
    "iris_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define feature and target objects\n",
    "x_feature = iris_dataset.data\n",
    "y_target = iris_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an object & assign it two sets of values for prediction\n",
    "x_new = [[3, 5, 4, 1],[5, 3, 4, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the logistic regression estimator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit data into the Logistic regression estimator\n",
    "logreg.fit(x_feature, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict the outcome using Logistic regression estimator\n",
    "logreg.predict(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library for model persistence\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x80\\x04\\x95\\x04\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x1esklearn.linear_model._logistic\\x94\\x8c\\x12LogisticRegression\\x94\\x93\\x94)\\x81\\x94}\\x94(\\x8c\\x07penalty\\x94\\x8c\\x02l2\\x94\\x8c\\x04dual\\x94\\x89\\x8c\\x03tol\\x94G?\\x1a6\\xe2\\xeb\\x1cC-\\x8c\\x01C\\x94G?\\xf0\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\rfit_intercept\\x94\\x88\\x8c\\x11intercept_scaling\\x94K\\x01\\x8c\\x0cclass_weight\\x94N\\x8c\\x0crandom_state\\x94N\\x8c\\x06solver\\x94\\x8c\\x05lbfgs\\x94\\x8c\\x08max_iter\\x94Kd\\x8c\\x0bmulti_class\\x94\\x8c\\x04auto\\x94\\x8c\\x07verbose\\x94K\\x00\\x8c\\nwarm_start\\x94\\x89\\x8c\\x06n_jobs\\x94N\\x8c\\x08l1_ratio\\x94N\\x8c\\x0en_features_in_\\x94K\\x04\\x8c\\x08classes_\\x94\\x8c\\x15numpy.core.multiarray\\x94\\x8c\\x0c_reconstruct\\x94\\x93\\x94\\x8c\\x05numpy\\x94\\x8c\\x07ndarray\\x94\\x93\\x94K\\x00\\x85\\x94C\\x01b\\x94\\x87\\x94R\\x94(K\\x01K\\x03\\x85\\x94h\\x1c\\x8c\\x05dtype\\x94\\x93\\x94\\x8c\\x02i4\\x94\\x89\\x88\\x87\\x94R\\x94(K\\x03\\x8c\\x01<\\x94NNNJ\\xff\\xff\\xff\\xffJ\\xff\\xff\\xff\\xffK\\x00t\\x94b\\x89C\\x0c\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x94t\\x94b\\x8c\\x05coef_\\x94h\\x1bh\\x1eK\\x00\\x85\\x94h \\x87\\x94R\\x94(K\\x01K\\x03K\\x04\\x86\\x94h%\\x8c\\x02f8\\x94\\x89\\x88\\x87\\x94R\\x94(K\\x03h)NNNJ\\xff\\xff\\xff\\xffJ\\xff\\xff\\xff\\xffK\\x00t\\x94b\\x89C`&\\xb7\\x0c?\\xb0\\xc2\\xda\\xbf\\x8d\\x93\\x94D5\\xec\\xee?}\\x84\\xe7\\x9a\\x10+\\x04\\xc0p\\x94MitX\\xf1\\xbf-\\xbf\\xa5\\xc9+\\xf8\\xe0?\\x91\\xed\\xbeM\\x86\\x1f\\xd4\\xbfgF\\xb2Wn\\x88\\xc9\\xbf,\\xa5\\x96\\xa9YW\\xee\\xbf\\x13\\x1d\\xfbP\\x9d\\xb6\\xbc\\xbf\\xac\\x1c\\xb5\\x1dr\\xdc\\xe4\\xbf\\xaa\\xa8b\\x80\\x97\\xc3\\x05@\\x81s\\x0c\\x9f\\x10B\\x00@\\x94t\\x94b\\x8c\\nintercept_\\x94h\\x1bh\\x1eK\\x00\\x85\\x94h \\x87\\x94R\\x94(K\\x01K\\x03\\x85\\x94h4\\x89C\\x18G\\xc8\\xd1\\xf7\\xef\\xac#@\\xd1\\x13\\x84;\\x08\\xbf\\x01@<\\xce\\xb2\\x06\\xb2\\x1c(\\xc0\\x94t\\x94b\\x8c\\x07n_iter_\\x94h\\x1bh\\x1eK\\x00\\x85\\x94h \\x87\\x94R\\x94(K\\x01K\\x01\\x85\\x94h(\\x89C\\x04d\\x00\\x00\\x00\\x94t\\x94b\\x8c\\x10_sklearn_version\\x94\\x8c\\x060.23.2\\x94ub.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use dumps method to persist the model\n",
    "persist_model = pkl.dumps(logreg)\n",
    "persist_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['regresfilename.pkl']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use joblib.dump to persist the model to a file\n",
    "import joblib\n",
    "#from sklearn.externals import joblib will not work. It is for older version.\n",
    "\n",
    "joblib.dump(logreg, 'regresfilename.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use joblib.load to persist the model to a file\n",
    "#create new estimator from the saved model\n",
    "new_logreg_estimator = joblib.load('regresfilename.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view the new estimator\n",
    "new_logreg_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#validate and use new estimator to predict\n",
    "new_logreg_estimator.predict(x_new)  #this proves that the model can be created, trained & persistedfor future use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is important to evaluate a model to check and validate its quality and accuracy. Scikit-learn's __metrics__ function is used to evaluate the accuracy of your machine learning model's predictions. It can be used to evaluate Classification, Clustering and regression type models.\n",
    "- To check quality of Classification type models, you can use __metrics.accuracy_score__ & __metrics.average_precision_score__ methods.\n",
    "- For Clustering, use __metrics.adjusted_rand_score__ method.\n",
    "- For Regression, use __metrics.mean_absolute_error__, __metrics.mean_squared_error__ and __metrics.meadian_absolute_error__ methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
